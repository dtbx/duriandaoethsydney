# Everything having to LLM resources and parameterizing same
model:
  name: "Llama 3 Instruct"
  api_url: "https://curated.aleph.cloud/vm/055e1267fb63f5961e8aee890cfc3f61387deee79f37ce51a44b21feee57d40b/completion"
  engine: "llamacpp"

  # Max Token count of prompts. This should
  #  reflect the maximum context size of your model
  max_prompt_tokens: 16384
  # Maximum number of tokens the model should generate
  #  per completion.
  max_completion_tokens: 30

  # Model parameterizing
  temperature: 0.7
  sampler_order: [6, 0, 1, 3, 4, 2, 5]
  top_p: 0.9
  top_k: 40

# Agent prompt configuration
agent:
  # Max number of attempts to complete on a prompt
  #  Inscrease the increase the size of potential resonponses
  max_completion_tries: 60
  # Summary system Prompt Template. See the default template for further info
  summary_system_prompt_template: "./templates/summary_system.yaml"
  # Proposal system Prompt Template. See the default template for further info
  proposal_system_prompt_template: "./templates/proposal_system.yaml"

# ChatML configuration
prompt_format:
  user_prepend: "<|start_header_id|>"
  user_append: "<|end_header_id|>"
  line_separator: "\n"
  log_start: ""
  stop_sequences:
    # default
    - "<|eot_id|>"
    # additional
    - "<|endoftext|>"
    - "<|"
